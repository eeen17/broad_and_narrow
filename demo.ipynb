{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9783dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/common/ethanchang/broad_and_narrow/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.6.9: Fast Llama patching. Transformers: 4.53.0.\n",
      "   \\\\   /|    NVIDIA RTX 6000 Ada Generation. Num GPUs = 8. Max memory: 47.408 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-it-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba4d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_suite = [\n",
    "    {\n",
    "        \"name\": \"English to French Translation\",\n",
    "        \"instruction\": \"Translate the following from English to French.\",\n",
    "        \"examples\": [\n",
    "            \"the cat is black -> le chat est noir\",\n",
    "            \"the dog is white -> le chien est blanc\",\n",
    "            \"I love science -> j'aime la science\",\n",
    "            \"are you my friend? ->\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Country to Capital\",\n",
    "        \"instruction\": \"What is the capital of the following country?\",\n",
    "        \"examples\": [\n",
    "            \"Japan -> Tokyo\",\n",
    "            \"Germany -> Berlin\",\n",
    "            \"Canada -> Ottawa\",\n",
    "            \"France -> \"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Simple Addition\",\n",
    "        \"instruction\": \"Calculate the sum of these two numbers.\",\n",
    "        \"examples\": [\n",
    "            \"5 + 8 -> 13\",\n",
    "            \"10 + 4 -> 14\",\n",
    "            \"7 + 2 -> 9\",\n",
    "            \"3 + 6 -> \"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Unscramble Words\",\n",
    "        \"instruction\": \"Unscramble the following letters to form a word.\",\n",
    "        \"examples\": [\n",
    "            \"pleap -> apple\",\n",
    "            \"nbaaan -> banana\",\n",
    "            \"rycher -> cherry\",\n",
    "            \"clinep -> \"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "\t\t\"name\": \"minus -> plus\",\n",
    "\t\t\"instruction\": \"Identify the pattern in the following math operations.\",\n",
    "\t\t\"examples\": [\n",
    "\t\t\t\"3 - 2 -> 5\",\n",
    "\t\t\t\"8 - 5 -> 13\",\n",
    "\t\t\t\"3 - 6 -> 9\",\n",
    "\t\t\t\"10 - 4 -> \"\n",
    "\t\t]\n",
    "\t}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e6092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing task: English to French Translation\n",
      "['Translate the following from English to French.', 'the cat is black -> le chat est noir', 'the dog is white -> le chien est blanc', \"I love science -> j'aime la science\", 'are you my friend? ->']\n",
      "\n",
      "Processing task: Country to Capital\n",
      "['What is the capital of the following country?', 'Japan -> Tokyo', 'Germany -> Berlin', 'Canada -> Ottawa', 'France -> ']\n",
      "\n",
      "Processing task: Simple Addition\n",
      "['Calculate the sum of these two numbers.', '5 + 8 -> 13', '10 + 4 -> 14', '7 + 2 -> 9', '3 + 6 -> ']\n",
      "\n",
      "Processing task: Unscramble Words\n",
      "['Unscramble the following letters to form a word.', 'pleap -> apple', 'nbaaan -> banana', 'rycher -> cherry', 'clinep -> ']\n",
      "\n",
      "Processing task: minus -> plus\n",
      "['Identify the pattern in the following math operations.', '3 - 2 -> 5', '8 - 5 -> 13', '3 - 6 -> 9', '10 - 4 -> ']\n"
     ]
    }
   ],
   "source": [
    "for task in task_suite:\n",
    "    task_name = task[\"name\"]\n",
    "    print(f\"\\nProcessing task: {task_name}\")\n",
    "\n",
    "    # # Clear previous run's data\n",
    "    # all_layer_outputs.clear()\n",
    "\n",
    "    # Prepare inputs for the current task\n",
    "    text_to_process = [task[\"instruction\"]] + task[\"examples\"]\n",
    "    print(text_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c3fc36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "TextStreamer only supports batch size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m tokenizer.pad_token = tokenizer.eos_token  \u001b[38;5;66;03m# Set padding token to EOS token\u001b[39;00m\n\u001b[32m      3\u001b[39m inputs = tokenizer(test, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m _ = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_streamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/common/ethanchang/broad_and_narrow/.venv/lib/python3.12/site-packages/unsloth/models/llama.py:1670\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   1669\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), torch.autocast(device_type = DEVICE_TYPE, dtype = dtype):\n\u001b[32m-> \u001b[39m\u001b[32m1670\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1673\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   1674\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   1675\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   1676\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/common/ethanchang/broad_and_narrow/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/common/ethanchang/broad_and_narrow/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2448\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2445\u001b[39m     input_ids = \u001b[38;5;28mself\u001b[39m.heal_tokens(input_ids, tokenizer)\n\u001b[32m   2447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streamer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2448\u001b[39m     \u001b[43mstreamer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2450\u001b[39m \u001b[38;5;66;03m# 6. Prepare `max_length` depending on other stopping criteria.\u001b[39;00m\n\u001b[32m   2451\u001b[39m input_ids_length = input_ids.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/common/ethanchang/broad_and_narrow/.venv/lib/python3.12/site-packages/transformers/generation/streamers.py:90\u001b[39m, in \u001b[36mTextStreamer.put\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03mReceives tokens, decodes them, and prints them to stdout as soon as they form entire words.\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value.shape) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m value.shape[\u001b[32m0\u001b[39m] > \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTextStreamer only supports batch size 1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value.shape) > \u001b[32m1\u001b[39m:\n\u001b[32m     92\u001b[39m     value = value[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: TextStreamer only supports batch size 1"
     ]
    }
   ],
   "source": [
    "test = ['Identify the pattern in the following math operations.', '3 - 2 -> 5', '8 - 5 -> 13', '3 - 6 -> 9', '10 - 4 -> ']\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "inputs = tokenizer(test, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.generation.streamers import TextStreamer\n",
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82be421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>human<|end_header_id|>\n",
      "\n",
      "Identify the pattern in the following math operations.\n",
      "3 - 2 -> 5\n",
      "8 - 5 -> 13\n",
      "3 - 6 -> 9\n",
      "10 - 4 -><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "To identify the pattern, let's analyze the operations:\n",
      "\n",
      "1. 3 - 2 = 1 (result is 1 more than 3)\n",
      "2. 8 - 5 = 3 (result is 3 more than 8)\n",
      "3. 3 - 6 = 3 (result is 3 more than 3)\n",
      "\n",
      "It seems that the pattern is adding the absolute difference of the two numbers to the larger number. \n",
      "\n",
      "So, if we apply this pattern to the next operation:\n",
      "\n",
      "10 - 4\n",
      "The absolute difference is 6 (|10 - 4| = 6)\n",
      "Add 6 to the larger number (10): 10 + 6 = 16\n",
      "\n",
      "Therefore, the result is 16.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "                               # EDIT HERE!\n",
    "    {\"from\": \"human\", \"value\" : \n",
    "\"\"\"Identify the pattern in the following math operations.\n",
    "3 - 2 -> 5\n",
    "8 - 5 -> 13\n",
    "3 - 6 -> 9\n",
    "10 - 4 ->\"\"\"\n",
    " \t}\n",
    "\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a65802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>human<|end_header_id|>\n",
      "\n",
      "Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're writing code, see Llama for an example implementation. If you're a user, please report this issue on GitHub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers. The next numbers in the sequence would be:\n",
      "\n",
      "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,...<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "                               # EDIT HERE!\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Describe the tallest tower in the world.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"What is Unsloth?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": 'You are a mathematician. Assuming that all numbers are in base-10 where the digits are \"0123456789\", what is 760+587? Let\\'s think step by step, and end the response with the result in \"\\\\boxed{result}\".'},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae7f9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "broad-and-narrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
